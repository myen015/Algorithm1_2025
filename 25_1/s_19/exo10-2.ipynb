{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG7ZUNjJ-MsM",
        "outputId": "b69499ec-bc85-4264-9a58-035bf5b77ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 1: P vs NP\n",
            "P problems: solvable in polynomial time\n",
            "Examples: linear search, sorting, BFS, DFS, Dijkstra\n",
            "\n",
            "NP / NP-complete problems: solutions verifiable in polynomial time\n",
            "Examples: Sudoku, 3-coloring, TSP, Clique\n",
            "\n",
            "NP-hard / Undecidable problems:\n",
            "Examples: Halting Problem, Busy Beaver\n",
            "\n",
            "Key idea about cryptography:\n",
            "- Easy to verify, hard to compute\n",
            "- If P = NP, cryptography would be broken\n",
            "\n",
            "Problem 2: Bayes theorem\n",
            "Probability patient actually has disease:\n",
            "9.02%\n",
            "Explanation: disease is rare, so false positives dominate\n",
            "\n",
            "Problem 3: Shannon Entropy\n",
            "Coin A (50/50): 1.0 bits\n",
            "Coin B (99/1): 0.08079313589591118 bits\n",
            "Coin C (1/99): 0.08079313589591118 bits\n",
            "\n",
            "Explanation:\n",
            "- Fair coin has maximum uncertainty → 1 bit\n",
            "- Biased coin is predictable → very low entropy\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# Problem 1: P, NP overview (demonstration)\n",
        "\n",
        "def problem1():\n",
        "    print(\"Problem 1: P vs NP\")\n",
        "    print(\"P problems: solvable in polynomial time\")\n",
        "    print(\"Examples: linear search, sorting, BFS, DFS, Dijkstra\")\n",
        "    print()\n",
        "\n",
        "    print(\"NP / NP-complete problems: solutions verifiable in polynomial time\")\n",
        "    print(\"Examples: Sudoku, 3-coloring, TSP, Clique\")\n",
        "    print()\n",
        "\n",
        "    print(\"NP-hard / Undecidable problems:\")\n",
        "    print(\"Examples: Halting Problem, Busy Beaver\")\n",
        "    print()\n",
        "\n",
        "    print(\"Key idea about cryptography:\")\n",
        "    print(\"- Easy to verify, hard to compute\")\n",
        "    print(\"- If P = NP, cryptography would be broken\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Problem 2: Bayes theorem\n",
        "def bayes_probability(prevalence, sensitivity, specificity):\n",
        "    \"\"\"\n",
        "    Computes P(disease | positive test)\n",
        "    \"\"\"\n",
        "    true_positive = sensitivity * prevalence\n",
        "    false_positive = (1 - specificity) * (1 - prevalence)\n",
        "    return true_positive / (true_positive + false_positive)\n",
        "\n",
        "\n",
        "def problem2():\n",
        "    print(\"Problem 2: Bayes theorem\")\n",
        "\n",
        "    prevalence = 0.001       # 0.1%\n",
        "    sensitivity = 0.99       # 99%\n",
        "    specificity = 0.99       # 99%\n",
        "\n",
        "    prob = bayes_probability(prevalence, sensitivity, specificity)\n",
        "\n",
        "    print(\"Probability patient actually has disease:\")\n",
        "    print(f\"{prob * 100:.2f}%\")\n",
        "    print(\"Explanation: disease is rare, so false positives dominate\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Problem 3: Shannon Entropy\n",
        "def entropy(probabilities):\n",
        "    \"\"\"\n",
        "    Computes Shannon entropy in bits\n",
        "    H(X) = - sum(p * log2(p))\n",
        "    \"\"\"\n",
        "    h = 0.0\n",
        "    for p in probabilities:\n",
        "        if p > 0:\n",
        "            h -= p * math.log2(p)\n",
        "    return h\n",
        "\n",
        "\n",
        "def problem3():\n",
        "    print(\"Problem 3: Shannon Entropy\")\n",
        "\n",
        "    # Coin A: fair\n",
        "    coin_A = [0.5, 0.5]\n",
        "\n",
        "    # Coin B: 99% heads\n",
        "    coin_B = [0.99, 0.01]\n",
        "\n",
        "    # Coin C: 1% heads\n",
        "    coin_C = [0.01, 0.99]\n",
        "\n",
        "    print(\"Coin A (50/50):\", entropy(coin_A), \"bits\")\n",
        "    print(\"Coin B (99/1):\", entropy(coin_B), \"bits\")\n",
        "    print(\"Coin C (1/99):\", entropy(coin_C), \"bits\")\n",
        "    print()\n",
        "\n",
        "    print(\"Explanation:\")\n",
        "    print(\"- Fair coin has maximum uncertainty → 1 bit\")\n",
        "    print(\"- Biased coin is predictable → very low entropy\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Run all problems\n",
        "if __name__ == \"__main__\":\n",
        "    problem1()\n",
        "    problem2()\n",
        "    problem3()\n"
      ]
    }
  ]
}