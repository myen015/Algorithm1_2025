*Problem1 Fibonacci*
Question:
Use Master Theorem to discuss the complexity of this decomposition and show, explain why time complexity is then logâ‚‚(n).

**Answer:**
When computing Fibonacci numbers with matrix exponentiation, we repeatedly divide the power n by 2.
Each step performs one or two constant-time 2Ã—2 matrix multiplications.
Therefore the recurrence relation for the running time is:

**ğ‘‡(ğ‘›)=ğ‘‡(ğ‘›/2)+ğ‘‚(1)**

Here we have one recursive subproblem of size n/2 (ğ‘=1, ğ‘=2) and a constant amount of extra work (ğ‘“(ğ‘›)=ğ‘‚(1)).
According to the Master Theorem, since

ğ‘›logğ‘ğ‘=ğ‘›log21=ğ‘›0=1 nlogb
	â€‹
and f(n)=O(1)=O(nlogba),
it follows that:

ğ‘‡(ğ‘›)=ğ‘‚(logğ‘›)


Thus the time complexity of computing ğ¹ğ‘› using matrix exponentiation (either [1,1;1,0]ğ‘› or 
[2,1;1,1]ğ‘›/2) is logarithmic in n, i.e. O(logâ‚‚ n).



*Problem 2 â€” 0/1 Knapsack*

**Why not Greedy?**

Greedy doesnâ€™t work because picking the most valuable or lightest item first doesnâ€™t always give the best total.
Choices affect each other, so we need to check combinations â†’ use **Dynamic Programming**.


We use DP to find the best value for each possible weight.
Formula:
[dp[w] = \max(dp[w],\ dp[w - weight_i] + value_i)]
Example: weights = [2,3,4,5], values = [3,4,5,6], W = 5 â†’ best value = **7**.


Use only one array `dp[W+1]` and loop backward.
Space becomes **O(W)**.

*Problem 3 â€” Neuro Computing*

**Random Binary Vectors**

We create 100 random binary vectors of length N, for example:
`[1, 0, 0, 1, 0, 1, ...]`.
Each vector is just a set of zeros and ones, like neuron signals.


**Similarity Functions**

To see how similar two vectors are, we calculate a **similarity**.
There are two types:

* **sim(x, y)** â€” normal dot product divided by the number of ones.
* **Jaccard(x, y)** â€” number of positions where both have 1 divided by total number of ones in both.

When we calculate many pairs, the similarity values look like a **bell curve** (Gaussian distribution).


**When N gets larger**

If N becomes bigger, the random vectors become more different from each other.
Their similarity gets smaller and the distribution becomes narrower.


**Large and Sparse Vectors**

If we take a very large vector, for example N = 2000 with only 5 ones,
then the number of possible such vectors is huge:
C(2000, 5).
That shows how many different patterns can exist.


**5Capacity**

â€œCapacityâ€ means how many different vectors (or patterns) can be stored
without mixing them up.
When vectors are long and sparse (few ones), the capacity becomes higher.

