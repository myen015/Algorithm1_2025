{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8cbbe1-b8f8-45bc-80e2-7569e9ac78cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: Huffman coding works on tokens with different frequencies.\n",
      "Huffman int tokens: [61, 21, 181, 140, 101, 15, 128]\n",
      "Length (Huffman): 7\n",
      "Recovered text: huffman coding works on tokens with different frequencies .\n",
      "\n",
      "Word embedding demo (first 5 words):\n",
      "a: 6 non-zero dims\n",
      "-: 6 non-zero dims\n",
      "huffman: 5 non-zero dims\n",
      ".: 5 non-zero dims\n",
      "word: 4 non-zero dims\n",
      "\n",
      "Dataset sizes: train=1591, test=398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification comparison on real text dataset (TF-IDF + LinearSVC) ===\n",
      "Baseline accuracy: 0.9347\n",
      "Huffman  accuracy: 0.6658\n",
      "Average seq length (baseline train): 203.61\n",
      "Average seq length (Huffman  train): 240.91\n",
      "Compression ratio (base / huff): 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, List, Tuple, Iterable\n",
    "from collections import Counter, defaultdict\n",
    "import heapq\n",
    "import math\n",
    "import re\n",
    "\n",
    "# ML / data\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 0. Простая токенизация по словам\n",
    "# ======================================================\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Простейший токенизатор: слова + знаки препинания, всё в lower.\"\"\"\n",
    "    return WORD_RE.findall(text.lower())\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1. Обобщённый Хаффман (для слов)\n",
    "# ======================================================\n",
    "\n",
    "@dataclass(order=True)\n",
    "class HuffmanNode:\n",
    "    freq: int\n",
    "    token: Optional[str] = field(compare=False, default=None)\n",
    "    left: Optional[\"HuffmanNode\"] = field(compare=False, default=None)\n",
    "    right: Optional[\"HuffmanNode\"] = field(compare=False, default=None)\n",
    "\n",
    "\n",
    "def build_frequency_table_tokens(tokens: Iterable[str]) -> Dict[str, int]:\n",
    "    return dict(Counter(tokens))\n",
    "\n",
    "\n",
    "def build_huffman_tree(freq_table: Dict[str, int]) -> HuffmanNode:\n",
    "    heap: List[HuffmanNode] = [HuffmanNode(f, t) for t, f in freq_table.items()]\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    # Если только один уникальный токен\n",
    "    if len(heap) == 1:\n",
    "        node = heap[0]\n",
    "        return HuffmanNode(node.freq, None, node, None)\n",
    "\n",
    "    while len(heap) > 1:\n",
    "        n1 = heapq.heappop(heap)\n",
    "        n2 = heapq.heappop(heap)\n",
    "        merged = HuffmanNode(n1.freq + n2.freq, None, n1, n2)\n",
    "        heapq.heappush(heap, merged)\n",
    "\n",
    "    return heap[0]\n",
    "\n",
    "\n",
    "def build_code_table_tokens(root: HuffmanNode) -> Dict[str, str]:\n",
    "    code: Dict[str, str] = {}\n",
    "\n",
    "    def traverse(node: HuffmanNode, prefix: str) -> None:\n",
    "        if node.token is not None:\n",
    "            code[node.token] = prefix or \"0\"\n",
    "            return\n",
    "        if node.left:\n",
    "            traverse(node.left, prefix + \"0\")\n",
    "        if node.right:\n",
    "            traverse(node.right, prefix + \"1\")\n",
    "\n",
    "    traverse(root, \"\")\n",
    "    return code\n",
    "\n",
    "\n",
    "def huffman_encode_tokens(tokens: List[str], code_table: Dict[str, str]) -> str:\n",
    "    return \"\".join(code_table[t] for t in tokens)\n",
    "\n",
    "\n",
    "def huffman_decode_tokens(bits: str, root: HuffmanNode) -> List[str]:\n",
    "    res: List[str] = []\n",
    "    node = root\n",
    "    for b in bits:\n",
    "        node = node.left if b == \"0\" else node.right\n",
    "        if node.token is not None:\n",
    "            res.append(node.token)\n",
    "            node = root\n",
    "    return res\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2. Биты -> целые токены, и обратно\n",
    "# ======================================================\n",
    "\n",
    "def bits_to_int_tokens(bits: str, chunk_size: int = 8) -> Tuple[List[int], int]:\n",
    "    \"\"\"\n",
    "    Битовая строка -> список целых токенов фиксированного размера.\n",
    "    Возвращаем (tokens, padding), где padding — сколько нулей дописали.\n",
    "    \"\"\"\n",
    "    tokens: List[int] = []\n",
    "    padding = (chunk_size - (len(bits) % chunk_size)) % chunk_size\n",
    "    bits_padded = bits + \"0\" * padding\n",
    "    for i in range(0, len(bits_padded), chunk_size):\n",
    "        chunk = bits_padded[i:i + chunk_size]\n",
    "        tokens.append(int(chunk, 2))\n",
    "    return tokens, padding\n",
    "\n",
    "\n",
    "def int_tokens_to_bits(tokens: List[int], padding: int, chunk_size: int = 8) -> str:\n",
    "    bits = \"\".join(f\"{t:0{chunk_size}b}\" for t in tokens)\n",
    "    if padding:\n",
    "        bits = bits[:-padding]\n",
    "    return bits\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3. Word-level Huffman Tokenizer (с UNK)\n",
    "# ======================================================\n",
    "\n",
    "class WordHuffmanTokenizer:\n",
    "    \"\"\"\n",
    "    Строит Хаффман по словам корпуса и кодирует новые тексты\n",
    "    в последовательности целочисленных токенов (по битам).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_texts: List[str], chunk_size: int = 8, unk_token: str = \"<unk>\"):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.UNK = unk_token\n",
    "\n",
    "        all_tokens: List[str] = []\n",
    "        for txt in corpus_texts:\n",
    "            all_tokens.extend(tokenize(txt))\n",
    "\n",
    "        freq_table = build_frequency_table_tokens(all_tokens)\n",
    "\n",
    "        # UNK для OOV-слов\n",
    "        if self.UNK not in freq_table:\n",
    "            freq_table[self.UNK] = 1\n",
    "\n",
    "        self.freq_table = freq_table\n",
    "        self.tree = build_huffman_tree(self.freq_table)\n",
    "        self.code_table = build_code_table_tokens(self.tree)\n",
    "\n",
    "    def encode_to_int_tokens(self, text: str) -> Tuple[List[int], int]:\n",
    "        tokens = tokenize(text)\n",
    "        mapped = [t if t in self.code_table else self.UNK for t in tokens]\n",
    "        bits = huffman_encode_tokens(mapped, self.code_table)\n",
    "        return bits_to_int_tokens(bits, self.chunk_size)\n",
    "\n",
    "    def decode_from_int_tokens(self, int_tokens: List[int], padding: int) -> str:\n",
    "        bits = int_tokens_to_bits(int_tokens, padding, self.chunk_size)\n",
    "        tokens = huffman_decode_tokens(bits, self.tree)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4. Co-occurrence word embeddings (как было)\n",
    "# ======================================================\n",
    "\n",
    "def build_cooccurrence(\n",
    "    corpus_texts: List[str],\n",
    "    window_size: int = 1,\n",
    "    min_count: int = 1,\n",
    ") -> Tuple[Dict[str, Dict[str, float]], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Двухпроходное построение co-occurrence:\n",
    "    1) считаем глобальные частоты,\n",
    "    2) считаем соседей только для слов с freq >= min_count.\n",
    "    \"\"\"\n",
    "    token_counts: Counter = Counter()\n",
    "    for txt in corpus_texts:\n",
    "        token_counts.update(tokenize(txt))\n",
    "\n",
    "    cooc: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    for txt in corpus_texts:\n",
    "        tokens = tokenize(txt)\n",
    "        n = len(tokens)\n",
    "        for i, w in enumerate(tokens):\n",
    "            if token_counts[w] < min_count:\n",
    "                continue\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(n, i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                c = tokens[j]\n",
    "                cooc[w][c] += 1.0\n",
    "\n",
    "    return cooc, dict(token_counts)\n",
    "\n",
    "\n",
    "def apply_tfidf_like_weights(\n",
    "    cooc: Dict[str, Dict[str, float]],\n",
    "    token_counts: Dict[str, int],\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    weighted: Dict[str, Dict[str, float]] = {}\n",
    "    for w, ctxs in cooc.items():\n",
    "        w_vec: Dict[str, float] = {}\n",
    "        for c, cnt in ctxs.items():\n",
    "            df = token_counts.get(c, 1)\n",
    "            w_vec[c] = cnt / math.log(1.0 + df)\n",
    "        weighted[w] = w_vec\n",
    "    return weighted\n",
    "\n",
    "\n",
    "def build_dense_embedding_matrix(\n",
    "    weighted_cooc: Dict[str, Dict[str, float]],\n",
    "    top_k: int = 500,\n",
    ") -> Tuple[List[str], List[List[float]]]:\n",
    "    sorted_words = sorted(\n",
    "        weighted_cooc.keys(),\n",
    "        key=lambda w: -len(weighted_cooc[w])\n",
    "    )[:top_k]\n",
    "\n",
    "    context_vocab = set()\n",
    "    for w in sorted_words:\n",
    "        context_vocab.update(weighted_cooc[w].keys())\n",
    "    context_vocab = sorted(context_vocab)\n",
    "    ctx_index = {c: i for i, c in enumerate(context_vocab)}\n",
    "\n",
    "    matrix: List[List[float]] = []\n",
    "    for w in sorted_words:\n",
    "        vec = [0.0] * len(ctx_index)\n",
    "        for c, val in weighted_cooc[w].items():\n",
    "            j = ctx_index.get(c)\n",
    "            if j is not None:\n",
    "                vec[j] = val\n",
    "        matrix.append(vec)\n",
    "\n",
    "    return sorted_words, matrix\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5. Tokenization pipelines for classification\n",
    "# ======================================================\n",
    "\n",
    "def baseline_word_tokenizer(corpus_texts: List[str], min_freq: int = 3) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Строим базовый токенизатор: слово -> индекс.\n",
    "    0 оставляем под PAD/UNK.\n",
    "    min_freq — минимальная частота слова в train, чтобы попасть в словарь.\n",
    "    \"\"\"\n",
    "    counts: Counter = Counter()\n",
    "    for txt in corpus_texts:\n",
    "        counts.update(tokenize(txt))\n",
    "\n",
    "    vocab: Dict[str, int] = {}\n",
    "    for tok, cnt in counts.items():\n",
    "        if cnt >= min_freq:\n",
    "            vocab[tok] = len(vocab) + 1  # 1..|V|\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def baseline_encode(text: str, vocab: Dict[str, int]) -> List[int]:\n",
    "    return [vocab.get(t, 0) for t in tokenize(text)]  # 0 = UNK\n",
    "\n",
    "\n",
    "def prepare_dataset_baseline(\n",
    "    texts: List[str],\n",
    "    vocab: Dict[str, int],\n",
    ") -> List[List[int]]:\n",
    "    return [baseline_encode(t, vocab) for t in texts]\n",
    "\n",
    "\n",
    "def prepare_dataset_huffman(\n",
    "    texts: List[str],\n",
    "    huffman_tok: WordHuffmanTokenizer,\n",
    ") -> Tuple[List[List[int]], List[int]]:\n",
    "    seqs: List[List[int]] = []\n",
    "    pads: List[int] = []\n",
    "    for t in texts:\n",
    "        ints, pad = huffman_tok.encode_to_int_tokens(t)\n",
    "        seqs.append(ints)\n",
    "        pads.append(pad)\n",
    "    return seqs, pads\n",
    "\n",
    "\n",
    "def seqs_to_bow(seqs: List[List[int]], vocab_size: int) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Bag-of-Tokens (частотный вектор) в виде разреженной матрицы.\n",
    "    Для baseline vocab_size = |V|+1.\n",
    "    \"\"\"\n",
    "    X = lil_matrix((len(seqs), vocab_size), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for tok in s:\n",
    "            if 0 <= tok < vocab_size:\n",
    "                X[i, tok] += 1.0\n",
    "    return X.tocsr()\n",
    "\n",
    "\n",
    "# ---------- стабильный хэш для n-грамм байтов ----------\n",
    "\n",
    "def stable_hash_ints(vals: Tuple[int, ...]) -> int:\n",
    "    \"\"\"\n",
    "    Простая детерминированная хэш-функция для кортежа целых.\n",
    "    \"\"\"\n",
    "    h = 2166136261  # FNV-like\n",
    "    for v in vals:\n",
    "        h ^= (v + 0x9e3779b9) & 0xFFFFFFFF\n",
    "        h = (h * 16777619) & 0xFFFFFFFF\n",
    "    return h\n",
    "\n",
    "\n",
    "def seqs_to_hashed_ngram_features(\n",
    "    seqs: List[List[int]],\n",
    "    n_features: int = 50000,\n",
    "    ngram_min: int = 1,\n",
    "    ngram_max: int = 3,\n",
    ") -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Признаки по Huffman-последовательностям:\n",
    "    считаем n-граммы байтов (1..3) и хэшируем их в пространство размерности n_features.\n",
    "    Аналог HashingVectorizer, но для целочисленных токенов.\n",
    "    \"\"\"\n",
    "    X = lil_matrix((len(seqs), n_features), dtype=np.float32)\n",
    "\n",
    "    for i, s in enumerate(seqs):\n",
    "        L = len(s)\n",
    "        if L == 0:\n",
    "            continue\n",
    "        for n in range(ngram_min, ngram_max + 1):\n",
    "            if L < n:\n",
    "                continue\n",
    "            for j in range(L - n + 1):\n",
    "                ng = tuple(s[j:j + n])          # n соседних байтов\n",
    "                key = (n,) + ng                 # различаем 1/2/3-граммы\n",
    "                h = stable_hash_ints(key) % n_features\n",
    "                X[i, h] += 1.0\n",
    "\n",
    "    return X.tocsr()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 6. Демо: кодек + эмбеддинги\n",
    "# ======================================================\n",
    "\n",
    "def run_demo() -> None:\n",
    "    corpus = [\n",
    "        \"This is a simple example of Huffman word-level compression.\",\n",
    "        \"Huffman coding works on tokens with different frequencies.\",\n",
    "        \"We can use Huffman codes as a tokenizer before a downstream model.\",\n",
    "        \"Neighbour-based co-occurrence can be used to create word embeddings.\",\n",
    "        \"Compression and tokenization are important for efficient NLP models.\",\n",
    "    ]\n",
    "\n",
    "    huff_tok = WordHuffmanTokenizer(corpus_texts=corpus, chunk_size=8)\n",
    "\n",
    "    sample_text = \"Huffman coding works on tokens with different frequencies.\"\n",
    "    ints, pad = huff_tok.encode_to_int_tokens(sample_text)\n",
    "    print(\"Sample text:\", sample_text)\n",
    "    print(\"Huffman int tokens:\", ints)\n",
    "    print(\"Length (Huffman):\", len(ints))\n",
    "\n",
    "    recovered = huff_tok.decode_from_int_tokens(ints, pad)\n",
    "    print(\"Recovered text:\", recovered)\n",
    "\n",
    "    cooc, token_counts = build_cooccurrence(corpus, window_size=1, min_count=1)\n",
    "    weighted = apply_tfidf_like_weights(cooc, token_counts)\n",
    "    vocab_list, emb_matrix = build_dense_embedding_matrix(weighted, top_k=20)\n",
    "\n",
    "    print(\"\\nWord embedding demo (first 5 words):\")\n",
    "    for w, vec in list(zip(vocab_list, emb_matrix))[:5]:\n",
    "        nonzeros = [(i, v) for i, v in enumerate(vec) if abs(v) > 1e-8]\n",
    "        print(f\"{w}: {len(nonzeros)} non-zero dims\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 7. Классификация на ДАТАСЕТЕ\n",
    "# ======================================================\n",
    "\n",
    "def load_binary_text_dataset() -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Берём 2 категории из 20newsgroups и получаем бинарный текстовый датасет.\n",
    "    \"\"\"\n",
    "    categories = [\"rec.autos\", \"rec.sport.hockey\"]\n",
    "\n",
    "    data = fetch_20newsgroups(\n",
    "        subset=\"all\",\n",
    "        categories=categories,\n",
    "        remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    )\n",
    "\n",
    "    texts: List[str] = data.data\n",
    "    labels: np.ndarray = data.target  # 0/1\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def run_classification_comparison_dataset() -> None:\n",
    "    \"\"\"\n",
    "    Сравнение baseline word-токенизатора и Huffman-токенизатора\n",
    "    на реальном текстовом датасете.\n",
    "\n",
    "    Baseline: word-индексы -> BOW -> TF-IDF -> LinearSVC.\n",
    "    Huffman: Huffman-байты -> hashed byte n-grams (1..3) -> TF-IDF -> LinearSVC.\n",
    "    \"\"\"\n",
    "    texts, labels = load_binary_text_dataset()\n",
    "\n",
    "    train_texts, test_texts, y_train, y_test = train_test_split(\n",
    "        texts,\n",
    "        labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDataset sizes: train={len(train_texts)}, test={len(test_texts)}\")\n",
    "\n",
    "    # ----- 1) Baseline -----\n",
    "    vocab = baseline_word_tokenizer(train_texts, min_freq=3)\n",
    "    train_seqs_base = prepare_dataset_baseline(train_texts, vocab)\n",
    "    test_seqs_base = prepare_dataset_baseline(test_texts, vocab)\n",
    "\n",
    "    baseline_vocab_size = len(vocab) + 1\n",
    "    X_train_base_counts = seqs_to_bow(train_seqs_base, baseline_vocab_size)\n",
    "    X_test_base_counts = seqs_to_bow(test_seqs_base, baseline_vocab_size)\n",
    "\n",
    "    tfidf_base = TfidfTransformer()\n",
    "    X_train_base = tfidf_base.fit_transform(X_train_base_counts)\n",
    "    X_test_base = tfidf_base.transform(X_test_base_counts)\n",
    "\n",
    "    clf_base = LinearSVC()\n",
    "    clf_base.fit(X_train_base, y_train)\n",
    "    y_pred_base = clf_base.predict(X_test_base)\n",
    "    acc_base = accuracy_score(y_test, y_pred_base)\n",
    "\n",
    "    # ----- 2) Huffman: hashed byte n-grams (1..3) -----\n",
    "    huff_tok = WordHuffmanTokenizer(train_texts, chunk_size=8)\n",
    "    train_seqs_huff, _ = prepare_dataset_huffman(train_texts, huff_tok)\n",
    "    test_seqs_huff, _ = prepare_dataset_huffman(test_texts, huff_tok)\n",
    "\n",
    "    n_features = 50000  # размерность пространства признаков для hashing-trick\n",
    "\n",
    "    X_train_huff_counts = seqs_to_hashed_ngram_features(\n",
    "        train_seqs_huff,\n",
    "        n_features=n_features,\n",
    "        ngram_min=1,\n",
    "        ngram_max=3,\n",
    "    )\n",
    "    X_test_huff_counts = seqs_to_hashed_ngram_features(\n",
    "        test_seqs_huff,\n",
    "        n_features=n_features,\n",
    "        ngram_min=1,\n",
    "        ngram_max=3,\n",
    "    )\n",
    "\n",
    "    tfidf_huff = TfidfTransformer()\n",
    "    X_train_huff = tfidf_huff.fit_transform(X_train_huff_counts)\n",
    "    X_test_huff = tfidf_huff.transform(X_test_huff_counts)\n",
    "\n",
    "    # Чуть ослабляем регуляризацию (C побольше),\n",
    "    # чтобы модель могла лучше использовать много признаков\n",
    "    clf_huff = LinearSVC(C=2.0)\n",
    "    clf_huff.fit(X_train_huff, y_train)\n",
    "    y_pred_huff = clf_huff.predict(X_test_huff)\n",
    "    acc_huff = accuracy_score(y_test, y_pred_huff)\n",
    "\n",
    "    # ----- длины последовательностей -----\n",
    "    def avg_len(seqs: List[List[int]]) -> float:\n",
    "        return sum(len(s) for s in seqs) / len(seqs)\n",
    "\n",
    "    avg_len_base = avg_len(train_seqs_base)\n",
    "    avg_len_huff = avg_len(train_seqs_huff)\n",
    "\n",
    "    print(\"\\n=== Classification comparison on real text dataset (TF-IDF + LinearSVC) ===\")\n",
    "    print(f\"Baseline accuracy: {acc_base:.4f}\")\n",
    "    print(f\"Huffman  accuracy: {acc_huff:.4f}\")\n",
    "    print(f\"Average seq length (baseline train): {avg_len_base:.2f}\")\n",
    "    print(f\"Average seq length (Huffman  train): {avg_len_huff:.2f}\")\n",
    "    print(f\"Compression ratio (base / huff): {avg_len_base / avg_len_huff:.2f}\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 8. Запуск демо\n",
    "# ======================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo()\n",
    "    run_classification_comparison_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9bbc30-acce-4de6-a4ca-fde5f348ffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sizes: train=1591, test=398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification comparison: Baseline vs Word-aligned Huffman ===\n",
      "Baseline accuracy: 0.9347\n",
      "Huffman  accuracy: 0.9372\n",
      "Average seq length (baseline train): 203.61\n",
      "Average seq length (Huffman  train): 203.61\n",
      "Vocab size (baseline): 7168\n",
      "Vocab size (Huffman):  17494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, List, Tuple, Iterable\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# ======================================================\n",
    "# 0. Токенизация\n",
    "# ======================================================\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Простейший токенизатор: слова + знаки препинания, всё в lower.\"\"\"\n",
    "    return WORD_RE.findall(text.lower())\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1. Хаффман-дерево по словам\n",
    "# ======================================================\n",
    "\n",
    "@dataclass(order=True)\n",
    "class HuffmanNode:\n",
    "    freq: int\n",
    "    token: Optional[str] = field(compare=False, default=None)\n",
    "    left: Optional[\"HuffmanNode\"] = field(compare=False, default=None)\n",
    "    right: Optional[\"HuffmanNode\"] = field(compare=False, default=None)\n",
    "\n",
    "\n",
    "def build_frequency_table_tokens(tokens: Iterable[str]) -> Dict[str, int]:\n",
    "    return dict(Counter(tokens))\n",
    "\n",
    "\n",
    "def build_huffman_tree(freq_table: Dict[str, int]) -> HuffmanNode:\n",
    "    heap: List[HuffmanNode] = [HuffmanNode(f, t) for t, f in freq_table.items()]\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    # Если только один уникальный токен\n",
    "    if len(heap) == 1:\n",
    "        node = heap[0]\n",
    "        return HuffmanNode(node.freq, None, node, None)\n",
    "\n",
    "    while len(heap) > 1:\n",
    "        n1 = heapq.heappop(heap)\n",
    "        n2 = heapq.heappop(heap)\n",
    "        merged = HuffmanNode(n1.freq + n2.freq, None, n1, n2)\n",
    "        heapq.heappush(heap, merged)\n",
    "\n",
    "    return heap[0]\n",
    "\n",
    "\n",
    "def build_code_table_tokens(root: HuffmanNode) -> Dict[str, str]:\n",
    "    \"\"\"token -> битовая строка Хаффмана.\"\"\"\n",
    "    code: Dict[str, str] = {}\n",
    "\n",
    "    def traverse(node: HuffmanNode, prefix: str) -> None:\n",
    "        if node.token is not None:\n",
    "            code[node.token] = prefix or \"0\"\n",
    "            return\n",
    "        if node.left:\n",
    "            traverse(node.left, prefix + \"0\")\n",
    "        if node.right:\n",
    "            traverse(node.right, prefix + \"1\")\n",
    "\n",
    "    traverse(root, \"\")\n",
    "    return code\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2. Baseline word-tokenizer\n",
    "# ======================================================\n",
    "\n",
    "def baseline_word_tokenizer(corpus_texts: List[str], min_freq: int = 3) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Строим базовый токенизатор: слово -> индекс.\n",
    "    0 оставляем под PAD/UNK.\n",
    "    min_freq — минимальная частота слова в train, чтобы попасть в словарь.\n",
    "    \"\"\"\n",
    "    counts: Counter = Counter()\n",
    "    for txt in corpus_texts:\n",
    "        counts.update(tokenize(txt))\n",
    "\n",
    "    vocab: Dict[str, int] = {}\n",
    "    for tok, cnt in counts.items():\n",
    "        if cnt >= min_freq:\n",
    "            vocab[tok] = len(vocab) + 1  # 1..|V|\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def baseline_encode(text: str, vocab: Dict[str, int]) -> List[int]:\n",
    "    return [vocab.get(t, 0) for t in tokenize(text)]  # 0 = UNK\n",
    "\n",
    "\n",
    "def prepare_dataset_baseline(\n",
    "    texts: List[str],\n",
    "    vocab: Dict[str, int],\n",
    ") -> List[List[int]]:\n",
    "    return [baseline_encode(t, vocab) for t in texts]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3. Word-aligned Huffman tokenizer\n",
    "# ======================================================\n",
    "\n",
    "class WordAlignedHuffmanTokenizer:\n",
    "    \"\"\"\n",
    "    Word-level Huffman: для каждого слова строим Хаффман-код,\n",
    "    но при классификации слово остаётся 1 токеном.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_texts: List[str], unk_token: str = \"<unk>\"):\n",
    "        self.UNK = unk_token\n",
    "\n",
    "        # 1) Собираем все токены из корпуса\n",
    "        all_tokens: List[str] = []\n",
    "        for txt in corpus_texts:\n",
    "            all_tokens.extend(tokenize(txt))\n",
    "\n",
    "        # 2) Частоты\n",
    "        freq_table = build_frequency_table_tokens(all_tokens)\n",
    "\n",
    "        # 3) UNK для OOV-слов\n",
    "        if self.UNK not in freq_table:\n",
    "            freq_table[self.UNK] = 1\n",
    "\n",
    "        self.freq_table = freq_table\n",
    "\n",
    "        # 4) Дерево Хаффмана и таблица кодов (token -> bits)\n",
    "        self.tree = build_huffman_tree(self.freq_table)\n",
    "        self.code_table = build_code_table_tokens(self.tree)\n",
    "\n",
    "        # 5) Словарь кодов: token -> целочисленный ID\n",
    "        #    (ID можно присваивать в порядке увеличения длины кода,\n",
    "        #     чтобы частые слова имели маленькие ID, как в реальных токенизаторах)\n",
    "        tokens_sorted = sorted(\n",
    "            self.code_table.keys(),\n",
    "            key=lambda t: (len(self.code_table[t]), self.code_table[t])\n",
    "        )\n",
    "\n",
    "        self.token_to_id: Dict[str, int] = {}\n",
    "        for idx, tok in enumerate(tokens_sorted, start=1):\n",
    "            self.token_to_id[tok] = idx\n",
    "\n",
    "        self.vocab_size: int = len(self.token_to_id) + 1  # +1 для 0=PAD/UNK\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Текст -> последовательность целочисленных токенов.\n",
    "        Каждый токен = одно слово, закодированное Хаффманом, но как 1 ID.\n",
    "        \"\"\"\n",
    "        toks = tokenize(text)\n",
    "        ids: List[int] = []\n",
    "        unk_id = self.token_to_id.get(self.UNK, 0)\n",
    "        for t in toks:\n",
    "            ids.append(self.token_to_id.get(t, unk_id))\n",
    "        return ids\n",
    "\n",
    "\n",
    "def prepare_dataset_huffman_word(\n",
    "    texts: List[str],\n",
    "    huff_tok: WordAlignedHuffmanTokenizer,\n",
    ") -> List[List[int]]:\n",
    "    return [huff_tok.encode(t) for t in texts]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4. Общая функция: seqs -> BOW\n",
    "# ======================================================\n",
    "\n",
    "def seqs_to_bow(seqs: List[List[int]], vocab_size: int) -> csr_matrix:\n",
    "    \"\"\"\n",
    "    Bag-of-Tokens (частотный вектор) в виде разреженной матрицы.\n",
    "    vocab_size = максимальный ID токена + 1.\n",
    "    \"\"\"\n",
    "    X = lil_matrix((len(seqs), vocab_size), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        for tok in s:\n",
    "            if 0 <= tok < vocab_size:\n",
    "                X[i, tok] += 1.0\n",
    "    return X.tocsr()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5. Датасет и сравнение\n",
    "# ======================================================\n",
    "\n",
    "def load_binary_text_dataset() -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Берём 2 категории из 20newsgroups и получаем бинарный текстовый датасет.\n",
    "    \"\"\"\n",
    "    categories = [\"rec.autos\", \"rec.sport.hockey\"]\n",
    "\n",
    "    data = fetch_20newsgroups(\n",
    "        subset=\"all\",\n",
    "        categories=categories,\n",
    "        remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    )\n",
    "\n",
    "    texts: List[str] = data.data\n",
    "    labels: np.ndarray = data.target  # 0/1\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def run_classification_comparison_word_aligned() -> None:\n",
    "    \"\"\"\n",
    "    Сравнение:\n",
    "      1) Baseline word-индексы\n",
    "      2) Word-aligned Huffman токены (одно слово = один токен),\n",
    "         где IDs присвоены с учётом длины Хаффман-кода.\n",
    "    Оба варианта используют BOW -> TF-IDF -> LinearSVC.\n",
    "    \"\"\"\n",
    "    texts, labels = load_binary_text_dataset()\n",
    "\n",
    "    train_texts, test_texts, y_train, y_test = train_test_split(\n",
    "        texts,\n",
    "        labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDataset sizes: train={len(train_texts)}, test={len(test_texts)}\")\n",
    "\n",
    "    # -------- Baseline --------\n",
    "    vocab = baseline_word_tokenizer(train_texts, min_freq=3)\n",
    "    train_seqs_base = prepare_dataset_baseline(train_texts, vocab)\n",
    "    test_seqs_base = prepare_dataset_baseline(test_texts, vocab)\n",
    "\n",
    "    baseline_vocab_size = len(vocab) + 1\n",
    "    X_train_base_counts = seqs_to_bow(train_seqs_base, baseline_vocab_size)\n",
    "    X_test_base_counts = seqs_to_bow(test_seqs_base, baseline_vocab_size)\n",
    "\n",
    "    tfidf_base = TfidfTransformer()\n",
    "    X_train_base = tfidf_base.fit_transform(X_train_base_counts)\n",
    "    X_test_base = tfidf_base.transform(X_test_base_counts)\n",
    "\n",
    "    clf_base = LinearSVC()\n",
    "    clf_base.fit(X_train_base, y_train)\n",
    "    y_pred_base = clf_base.predict(X_test_base)\n",
    "    acc_base = accuracy_score(y_test, y_pred_base)\n",
    "\n",
    "    # -------- Huffman word-aligned --------\n",
    "    huff_tok = WordAlignedHuffmanTokenizer(train_texts)\n",
    "    train_seqs_huff = prepare_dataset_huffman_word(train_texts, huff_tok)\n",
    "    test_seqs_huff = prepare_dataset_huffman_word(test_texts, huff_tok)\n",
    "\n",
    "    huffman_vocab_size = huff_tok.vocab_size\n",
    "    X_train_huff_counts = seqs_to_bow(train_seqs_huff, huffman_vocab_size)\n",
    "    X_test_huff_counts = seqs_to_bow(test_seqs_huff, huffman_vocab_size)\n",
    "\n",
    "    tfidf_huff = TfidfTransformer()\n",
    "    X_train_huff = tfidf_huff.fit_transform(X_train_huff_counts)\n",
    "    X_test_huff = tfidf_huff.transform(X_test_huff_counts)\n",
    "\n",
    "    clf_huff = LinearSVC()\n",
    "    clf_huff.fit(X_train_huff, y_train)\n",
    "    y_pred_huff = clf_huff.predict(X_test_huff)\n",
    "    acc_huff = accuracy_score(y_test, y_pred_huff)\n",
    "\n",
    "    # ----- длины последовательностей -----\n",
    "    def avg_len(seqs: List[List[int]]) -> float:\n",
    "        return sum(len(s) for s in seqs) / len(seqs)\n",
    "\n",
    "    avg_len_base = avg_len(train_seqs_base)\n",
    "    avg_len_huff = avg_len(train_seqs_huff)\n",
    "\n",
    "    print(\"\\n=== Classification comparison: Baseline vs Word-aligned Huffman ===\")\n",
    "    print(f\"Baseline accuracy: {acc_base:.4f}\")\n",
    "    print(f\"Huffman  accuracy: {acc_huff:.4f}\")\n",
    "    print(f\"Average seq length (baseline train): {avg_len_base:.2f}\")\n",
    "    print(f\"Average seq length (Huffman  train): {avg_len_huff:.2f}\")\n",
    "    print(f\"Vocab size (baseline): {baseline_vocab_size}\")\n",
    "    print(f\"Vocab size (Huffman):  {huffman_vocab_size}\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 6. Запуск\n",
    "# ======================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_classification_comparison_word_aligned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676397b-f680-4bef-aa5c-a8cf64c72bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
